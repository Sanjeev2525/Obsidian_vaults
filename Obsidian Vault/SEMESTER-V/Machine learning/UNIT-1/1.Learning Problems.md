# Machine learning
*A field of study that gives computer capability to learn without being explicitly programmed*

## why we need machine learning ?
1. Human expertise does not exist
2. Humans can't explore the expertise
3. Model must be customized
4. Model based on huge amount of data

## Successful application of ml

#### Speech recognition
- Outperform all other approaches that have been attempted to date

#### Data mining
- Learning algorithms being used to discover valuable knowledge from large commercial databases
- Detect fraudulent use of credit cards

#### Play Games
•Play backgammon at levels approaching the performance of human world champions.



---
---


# LEARNING PROGRAMS 

## DEFENITION 
A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.


### Examples:

#### A checkers learning problem:
Task T: Playing checkers
Performance measure P: Percent of games won against opponents
Training experience E: Playing practice games against itself

#### Handwriting recognition learning problem:
Task T: Recognizing and classifying handwritten words within images
Performance measure P: Percent of words correctly classified
Training experience E: A database of handwritten words with given classifications

### Types of learning :

#### 1. Supervised learning(pending)
#### 2. Unsupervised learning(pending)
#### 3. Reinforcement learning(pending)
---
--- 

# DESIGNING A LEARNING PROGRAM

`Consider designing a program to learn to play checkers, with the goal of entering it in the world checkers tournament`

## Requires the following sets :

### Choosing Training Experience

<font style="color:#C12869">Will the training experience provide direct or indirect feedback?</font>

- Direct Feedback: Immediate feedback and the correct move for each move

- Indirect Feedback: Feedback based move sequences and final outcomes of various games played.


<font style="color:#C12869">Degree to which the learner controls the sequence of training examples
</font>

Example teacher teaching :
- With trainer help
- With partial help
- With no help

<font style="color:#C12869">Distribution of examples </font>

How well the training experience represents the distribution of examples over which the final system performance P will be measured ?

- If training the checkers program consists only of experiences played against itself, it may never encounter crucial board states that are likely to be played by the human checkers champion

- Most theory of machine learning rests on the assumption that the distribution of training examples is identical to the distribution of test examples

---
### Partial Design of Checkers Learning Program

**A checkers learning problem:**
- Task T: playing checkers
- Performance measure P: percent of games won in the world tournament
- Training experience E: games played against itself
- Target Function: V: Board → *R*
- Target function representation: 
![[Pasted image 20220912212121.png]]


**Remaining choices**

- The exact type of knowledge to be learned
- A representation for this target knowledge
- A learning mechanism

---
### Choosing the Target Function

- Assume that you can determine legal moves
- Program needs to learn the best move from among legal moves
	- Defines large search space known a <font style="color:lightblue">Priori</font>

•<font style="color:crimson">Target function:</font> ChooseMove : B → M (ChooseMove is difficult to learn given indirect training)


<font style="color:pink">Alternative target function</font>

- An evaluation function that assigns a numerical score to any given board state

- V : B →  *R*    ( where  *R*   is the set of real numbers)
- V(b) for an arbitrary board state b in B
	- If b is a final board state that is won, then V(b) = 100
	- If b is a final board state that is lost, then V(b) = -100
	- If b is a final board state that is drawn, then V(b) = 0
	- If b is not a final state, then V(b) = V(b '), where b' is the best final board state that can be achieved starting from b and playing optimally until the end of the game

- V(b) gives a recursive definition for board state b
	- Not usable because not efficient to compute except is first three trivial cases
	- Nonoperational definition

- Goal of learning is to discover an operational description of V
- Learning the target function is often called function approximation

---
### Choosing the Representation of the Target Function

**Use linear combination of the following board features:**

- x1: the number of black pieces on the board
- x2: the number of red pieces on the board
- x3: the number of black kings on the board
- x4: the number of red kings on the board
- x5: the number of black pieces threatened by red (i.e. which can be captured on red's next turn)
- x6: the number of red pieces threatened by black
---
### Choosing the Function Approximation Algorithm

- To learn   $\hat{V}$  we require a set of training examples describing the board b and the training value $V_{train}(b)$.
- Ordered pair (b,$V_{train}(b)$ 

<<$x_1$ = 3 , $x_2$ =0 ,$x_3$ = 1,$x_4$ = 0 ,$x_5$ = 0 ,$x_6$ = 0 >+100> 

---
## Estimating Training Values
- Need to assign specific scores to intermediate board states
- Approximate intermediate board state b using the learner's current approximation of the next board state following b
$$V_{train}(b) <- \hat{V}(Successor(b))$$

- Simple and successful approach
- More accurate for states closer to end states
---

## Adjusting the Weights

- Choose the weights $w_i$ to best fit the set of training examples
- Minimize the squared error E between the train values and the values predicted by the hypothesis
$$E ≡ \sum {(V_{train}(b)- \hat{V}(b))}^2 $$

- Require an algorithm that
	- Will incrementally refine weights as new training examples become available
	- Will be robust to errors in these estimated training values

•Least Mean Squares (LMS) is one such algorithm

- For each train example (b,$V_{train}(b)$ 
	- Use the current weights to calculate
	- For each weight $w_i$ ,  update it as
		![[Pasted image 20220912220134.png]]
- where 
![[Pasted image 20220912220153.png]]
is a small constant (e.g. 0.1)

---
