**Delay:**
Packet starts in a host (the source), passes through a series of routers, and ends its journey in another host (the destination). As a packet travels from one node (host or router) to the subsequent node (host or router) along this path, the packet suffers from several types of delays at each node along the path. 

The most important of these delays are the 
- Nodal processing delay 
- Queuing delay, 
- Transmission delay,
- Propagation delay; 
    
    together, these delays accumulate to give a <font style="color:lightblue"> total nodal delay.</font>

![[Pasted image 20220812224639.png]]

**Processing Delay :**
The time required to examine the packet’s header and determine where to direct the packet is part of the processing delay. The processing delay can also include other factors, such as the time needed to check for bit-level errors in the packet that occurred in transmitting the packet’s bits from the upstream node to router A.

**Queuing Delay:**
At the queue, the packet experiences a queuing delay as it waits to be transmitted onto the link. The length of the queuing delay of a specific packet will depend on the number of earlier-arriving packets that are queued and waiting for transmission onto the link. If the queue is empty and no other packet is currently being transmitted, then our packet’s queuing delay will be zero. On the other hand, if the traffic is heavy and many other packets are also waiting to be transmitted, the queuing delay will be long.

**Transmission Delay:**
Assuming that packets are transmitted in a first-come-first-served manner, as is common in packet-switched networks, our packet can be transmitted only after all the packets that have arrived before it have been transmitted.

Denote the length of the packet by L bits, and denote the transmission rate of the link from router A to router B by R bits/sec. 
 
For example, for a 10 Mbps Ethernet link, the rate is R = 10 Mbps; for a 100 Mbps Ethernet link, the rate is R = 100 Mbps. 

The transmission delay is  $$\frac{L}{R}.$$  This is the amount of time required to push (that is, transmit) all of the packet’s bits into the link. Transmission delays are typically on the order of microseconds to milliseconds in practice.


**Propagation Delay:**

Once a bit is pushed into the link, it needs to propagate to router B. The time required to propagate from the beginning of the link to router B is the <font style="color:lightblue">propagation delay. </font>

The bit propagates at the propagation speed of the link. The propagation speed depends on the physical medium of the link (i.e, fiber optics, twisted-pair copper wire, and so on) and is in the range of 

2 x10<sup>8</sup> meters/sec to 3 x 10<sup>8</sup> meters/sec 

which is equal to, or a little less than, the speed of light. The propagation delay is the distance between two routers divided by the propagation speed. 

That is, the propagation delay is 
$$\frac{d}{s}$$
 - where "d" is the distance between router A and router B and 
 - "s" is the propagation speed of the link.

the total nodal delay is given by 

$$d_{nodal}=d_{proc}+d_{queue}+d_{trans}+d_{prop}$$

Comparing Transmission and Propagation Delay :

``Yet to be filled

Queuing Delay and Packet Loss :

<font style="color:yellow">Queuing Delay :</font>

 For example, if 10 packets arrive at an empty queue at the same time, the first packet transmitted will suffer no queuing delay, while the last packet transmitted will suffer a relatively large queuing delay (while it waits for the other nine packets to be transmitted).

<font style="color:violet">When is the queuing delay large and when is it insignificant?</font>

The answer to this question depends on the rate at which traffic arrives at the queue, the transmission rate of the link, and the nature of the arriving traffic, that is, whether the traffic arrives periodically or arrives in bursts. 

To gain some insight here, 

let <font style="color:red">'a'</font> denote the average rate at which packets arrive at the queue(pack/s) .
let <font style="color:red">'R'</font> is the transmission rate; (b/s) at which bits are pushed out of the queue. 
Also  all packets consist of <font style="color:red"> 'L'</font> bits. 

Then the average rate at which bits arrive at the queue is 
$$La.bits/sec$$ 

  The ratio 
  $$\frac{La}{R}$$ is called the <font style="color:green">traffic intensity</font>, often plays an important role in estimating the extent of the queuing delay. 
  
  If $$\frac{La}{R}>1$$  then the average rate at which bits arrive at the queue exceeds the rate at which the bits can be transmitted from the queue. In this unfortunate situation, the queue will tend to increase without bound and the queuing delay will approach infinity! Therefore, one of the golden rules in traffic engineering is: Design your system so that the traffic intensity is no greater than 1.
![[Pasted image 20220815191418.png]]

One important aspect of Figure 1.18 is the fact that as the traffic intensity approaches 1, the average queuing delay increases rapidly. A small percentage increase in the intensity will result in a much larger percentage-wise increase in delay. Perhaps you have experienced this phenomenon on the highway. If you regularly drive on a road that is typically congested, the fact that the road is typically congested means that its traffic intensity is close to 1. If some event causes an even slightly larger-than-usual amount of traffic, the delays you experience can be huge.

---

<font style="color:yellow">Packet loss :</font>

In reality a queue preceding a link has finite capacity, although the queuing capacity greatly depends on the router design and cost. Because the queue capacity is finite, packet delays do not really approach infinity as the traffic intensity approaches 1. Instead, a packet can arrive to find a full queue. With no place to store such a packet, a router will drop that packet; that is, the packet will be lost.

From an end-system viewpoint, a packet loss will look like a packet having been transmitted into the network core but never emerging from the network at the destination. The fraction of lost packets increases as the traffic intensity increases. Therefore, performance at a node is often measured not only in terms of delay, but also in terms of the probability of packet loss.

---
**End-to-End Delay:**

Let’s now consider the total delay from source to destination. 

To get a handle on this concept, suppose there are N - 1 routers between the source host and the destination host. 

Let’s also suppose for the moment that the network is uncongested (so that queuing delays are negligible), the processing delay at each router and at the source host is $d_{proc}$, the transmission rate out of each router and out of the source host is R bits/sec, and the propagation on each link is $d_{prop}$. 

The nodal delays accumulate and give an end-to-end delay,

  $d_{end-end} = N(d_{proc} + d_{trans} + d_{prop})$
    
where, once again,   
						 $d_{trans} = \frac{L}{R}$ 
									 where L is the packet size.

---

Traceroute :
` To be updated`

---

**Other Delays :**

- An end system wanting to transmit a packet into a shared medium (e.g., as in a WiFi or cable modem scenario) may purposefully delay its transmission as part of its protocol for sharing the medium with other end systems;

- Another important delay is media packetization delay, which is present in <font style="color:lightblue">Voiceover-IP (VoIP)</font> applications. In VoIP, the sending side must first fill a packet with encoded digitized speech before passing the packet to the Internet. This time to fill a packet—called the packetization delay—can be significant and can impact the user perceived quality of a VoIP call.

---

<font style="color:pink">Throughput in Computer Networks:</font>

In addition to delay and packet loss, another critical performance measure in computer networks is end-to-end throughput. 

To define throughput, consider transferring a large file from Host A to Host B across a computer network.  

The <font style="color:lightblue">instantaneous throughput</font> at any instant of time is the rate (in bits/sec) at which Host B is receiving the file.

If the file consists of F bits and the transfer takes T seconds for Host B to receive all F bits, then the average throughput of the file transfer is 
                                $\frac{F}{T}  bits/sec$

`FURTHER INSIGHTS:`

![[Pasted image 20220815214722.png]]

Figure 1.19 
(a) shows two end systems, a server and a client, connected by two communication links and a router. 

Consider the throughput for a file transfer from the server to the client. 
Let <font style="color:VIOLET">Rs</font> denote the rate of the link between the server and the router; 
and <font style="color:violet">Rc</font> denote the rate of the link between the router and the client. 

Suppose that the only bits being sent in the entire network are those from the server to the client. 

<font style="color:lightgreen">We now ask, in this ideal scenario, what is the server to-client throughput?</font>  

To answer this question, we may think of bits as fluid and communication links as pipes. 

If Rs < Rc, then the bits pumped by the server will “flow” right through the router and arrive at the client at a rate of Rs bps, giving a throughput of Rs bps. 

If, on the other hand, Rc < Rs, then the router will not be able to forward bits as quickly as it receives them. In this case, bits will only leave the router at rate Rc, giving an end-to-end throughput of Rc. 

``(Note also that if bits continue to arrive at the router at rate Rs, and continue to leave the router at Rc, the backlog of bits at the router waiting for transmission to the client will grow and grow—a most undesirable situation!)

Thus, for this simple two-link network, the throughput is <font style="color:yellow">min{Rc, Rs}</font>, that is, it is the transmission rate of the <font style="color:pink">bottleneck link.</font>

![[Pasted image 20220815215341.png]]

Figure 1.19(b) now shows a network with N links between the server and the client, with the transmission rates of the N links being R1, R2, c, RN. Applying the same analysis as for the two-link network, we find that the throughput for a file transfer from server to client is min{R1, R2, c, RN}, which is once again the transmission rate of the bottleneck link along the path between server and client.

![[Pasted image 20220816144251.png]]

<font style="color:lightgreen">But what if the rate of the common link is of the same order as Rs and Rc? What will the throughput be in this case?</font> 

Let’s take a look at a specific example. 
Suppose Rs = 2 Mbps, Rc = 1 Mbps, R = 5 Mbps, and the common link divides its transmission rate equally among the 10 downloads. 

Then the bottleneck for each download is no longer in the access network, but is now instead the shared link in the core, which only provides each download with 500 kbps of throughput. Thus, the end-to-end throughput for each download is now reduced to 500 kbps.

---
2
